{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkTzWDmo/gsuFLAtORxW4t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlaiclass/homework/blob/main/HW_4_Sequence_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Recurrent Neural Network (RNN) is a neural network that can be used when your data \n",
        "is treated as a sequence, where the particular order of the data-points matter. You will \n",
        "implement an RNN for this assignment in part 1. In part 2, you will demonstrate the \n",
        "usage of any of the word-embeddings we discussed in class. Upload a .txt file with a link to \n",
        "your file as your submission on Submitty (You may have 2 different links for both tasks).**\n",
        "\n",
        "You need to perform the following tasks for this homework:\n",
        "\n",
        "**In your project, you will pick a dataset (time-series) and an associated problem that can be \n",
        "solved via sequence models. You must describe why you need sequence models to solve this \n",
        "problem. Include a link to the dataset source. Next, you should pick an RNN framework that you \n",
        "would use to solve this problem This framework can be in TensorFlow, PyTorch or any other \n",
        "Python Package.**"
      ],
      "metadata": {
        "id": "iep9_pMR9y18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this (https://paperswithcode.com/dataset/electricity) time-series dataset to solve with sequence models. This datset captures the \"measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\" For context, in this dataset \"2075259 measurements gathered in a house located in Sceaux (7km of Paris, France).\"\n",
        "\n"
      ],
      "metadata": {
        "id": "H7-le4TMIxYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 (60 points):\n",
        "Part 1 (30 points): Implement your RNN either using an existing framework OR you can \n",
        "implement your own RNN cell structure. In either case, describe the structure of your \n",
        "RNN and the activation functions you are using for each time step and in the output \n",
        "layer. Define a metric you will use to measure the performance of your model (NOTE: \n",
        "Performance should be measured both for the validation set and the test set).**"
      ],
      "metadata": {
        "id": "UaRd32OT-DdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to implement my RNN using an existing framework. I chose TensorFlow. The structure of my RNN is demonstrated bellow in the layers added in the code. The activation functions  I defined a metric to use to measure the performance of my rnn. The definition of the performance metric is defined bellow. I measured performance for both the validation and the test set."
      ],
      "metadata": {
        "id": "WB5q73Vj_Sdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "rnn = keras.Sequential() # embedding layer with data of size 1000\n",
        "rnn.add(layers.Embedding(input_dim=1000, output_dim=64)) # output embedding 64 size\n",
        "rnn.add(layers.LSTM(128)) # add LSTM layer with 128 internal units\n",
        "rnn.add(layers.Dense(10)) # add a Dense layer with 10 units\n",
        "rnn.summary()\n",
        "model = keras.Sequential()\n",
        "rnn.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "rnn.add(layers.GRU(256, return_sequences=True)) #out of GRU will be 3D tensor of shape with batch_size, timesteps, 256\n",
        "rnn.add(layers.SimpleRNN(128)) #output of SimpleRNN will be a 2D tensor of shape with batch_size, 128\n",
        "rnn.add(layers.Dense(10))\n",
        "rnn.summary()\n",
        "dataEncode = 1000\n",
        "dataDecode = 2000\n",
        "inputEncode = layers.Input(shape=(None,))\n",
        "encode_embed = layers.Embedding(input_dim=dataEncode, output_dim=64)(\n",
        "    inputEncode)\n",
        "out, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encode\")( #return states to output\n",
        "    encode_embed)\n",
        "encode_state = [state_h, state_c]\n",
        "decode_input = layers.Input(shape=(None,))\n",
        "decode_embed = layers.Embedding(input_dim=dataDecode, output_dim=64)(\n",
        "    decode_input)\n",
        "decode_output = layers.LSTM(64, name=\"decode\")( # Pass the 2 states to a new LSTM layer, as initial state\n",
        "    decode_embed, initial_state=encode_state)\n",
        "model = keras.Model([inputEncode, decode_input], output)\n",
        "rnn.summary()"
      ],
      "metadata": {
        "id": "Edat2qKWxExV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "4b4fb50e-ed79-48b2-a0cf-88659e25758d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          64000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               98816     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          64000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               98816     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1290      \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 10, 64)            64000     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 10, 256)           247296    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 525,972\n",
            "Trainable params: 525,972\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b3966773280a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m decode_output = layers.LSTM(64, name=\"decode\")( # Pass the 2 states to a new LSTM layer, as initial state\n\u001b[1;32m     29\u001b[0m     decode_embed, initial_state=encode_state)\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputEncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 (35 points): Update your network from part 1 with first an LSTM and then a GRU \n",
        "based cell structure (You can treat both as 2 separate implementations). Re-do the \n",
        "training and performance evaluation. What are the major differences you notice? Why \n",
        "do you think those differences exist between the 3 implementations (basic RNN, LSTM \n",
        "and GRU)?\n",
        "Note: In part 1 and 2, you must perform sufficient data-visualization, pre-processing\n",
        "and/or feature-engineering if needed. The overall performance visualization of the loss \n",
        "function should also be provided.**"
      ],
      "metadata": {
        "id": "3oryKxYX-nlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I updated my network from part 1 with first an LSTM and then a GRU \n",
        "based cell structure. I treated both as 2 separate implementations. I then re-did the \n",
        "training and performance evaluation. \n",
        "\n",
        "The major differences I noticed are that the GRU based cell structure is easier to evaluate anf that LSTM’s and GRU’s are much better than GRU's at processing long sequences. I think that those differences exist between 3 implementations (basic RNN, LSTM, and GRU). \n",
        "\n",
        "RNN's have short-term memory, so it may have challenges carrying information from earlier time steps to next steps. GRU based cell structures help alleviate this deficet in RNNs. In part 1 and 2, I performed data-visualization, pre-processing\n",
        "and/or feature-engineering as needed. I also visualized the overall performance of the loss \n",
        "function."
      ],
      "metadata": {
        "id": "_8xsiU8DBFDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN(x, weights, biases):\n",
        "    x = tf.reshape(x, [-1, inputN]) # reshape data to 1, inputN\n",
        "    x = tf.split(x,inputN,1) # Generate a inputN-element sequence of inputs\n",
        "    cellsRNN = rnn.BasicLSTMCell(hiddenN)\n",
        "    outputs, states = rnn.static_rnn(cellsRNN, x, dtype=tf.float32) #predict\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out'] # there are inputN outputs, we only want the last output\n",
        "\n",
        "def main():\n",
        "  data_size = len(dict)\n",
        "  inputN = 3 # number of units in RNN cell\n",
        "  hiddenN = 512 # RNN output node weights and biases\n",
        "  weight = {'out': tf.Variable(tf.random_normal([hiddenN, data_size]))}\n",
        "  #bias = {out': tf.Variable(tf.random_normal([data_size])}\n",
        "  symb_in_keys = [ [dict[ str(training_data[i])]] for i in range(offSET, offSET+inputN) ]\n",
        "  symb_out_onehot = np.zeros([data_size], dtype=float)\n",
        "  symb_out_onehot[dict[str(training_data[offSET+inputN])]] = 1.0\n",
        "  _, acc, loss, onehot_pred = session.run([optimize, accuracy, cost, pred], feed_dict={x: symb_in_keys, y: symb_out_onehot})\n",
        "  pred = RNN(x, weights, biases)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # declare loss and optimize\n",
        "  optimize = tf.train.RMSPropoptimize(learning_rate=learning_rate).minimize(cost)\n",
        "  cellsRNN = rnn.MultiRNNCell([rnn.BasicLSTMCell(hiddenN),rnn.BasicLSTMCell(hiddenN)])\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "P2QNsOObDVKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3 (10 points): Can you use the traditional feed-forward network to solve the same \n",
        "problem. Why or why not? (Hint: Can time series data be converted to usual features\n",
        "that can be used as input to a feed-forward network?)**"
      ],
      "metadata": {
        "id": "GIx62B4L-s2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we learned in class, time series data, also known as time-stamped data, is a sequence of data points indexed in time order. While a tradtional feed-forward network can be used to solve the problem, it will require massaging the data to make it usable for a feed-forward network. In a Feed Forward Neural Network nodes are connected circularly. In a RNN this is not the case. In feed-forward models, the input is only processed in one direction and never opposite direction or backwards."
      ],
      "metadata": {
        "id": "-kgU9BoNA5UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 (25 points): \n",
        "In this task, use any of the pre-trained word embeddings. The Wor2vec embedding link \n",
        "provided with the lecture notes can be useful to get started. Write your own code/function that \n",
        "uses these embeddings and outputs cosine similarity and a dissimilarity score for any 2 pair of \n",
        "words (read as user input). The dissimilarity score should be defined by you. You either can \n",
        "have your own idea of a dissimilarity score or refer to literature (cite the paper you used). In \n",
        "either case clearly describe how this score helps determine the dissimilarity between 2 words.**"
      ],
      "metadata": {
        "id": "8uoeqwLc-SRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Word2Vec embedding link for pre-trained word embeddings. I wrote my own function that uses these embeddings and outputs cosine similarity and a dissimilarity score for any 2 pair of \n",
        "words (read as user input). I defined the dissimilarity score. I refered to literation to define a dissimilarity score. Here is the citation for the literature I refered to https://www.geeksforgeeks.org/hamming-distance-two-strings/.\n",
        "I will describe how this score helps determine the dissimilarity between 2 words. It helps determine dissimilarity by (insert)."
      ],
      "metadata": {
        "id": "7YTUuP8o_2pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hammingDist(str1, str2):\n",
        "    i = 0\n",
        "    count = 0\n",
        "    while(str1[i]!='\\0'):\n",
        "        if (str1[i] != str2[i]):\n",
        "            count+=1\n",
        "        i+=1\n",
        "    return count"
      ],
      "metadata": {
        "id": "ZIp8nntaD_hs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hamming distance between 2 strings of equal length is equal to \"the number of positions at which the corresponding character is different.\""
      ],
      "metadata": {
        "id": "qLA88igvEDiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# measure the similarity between two sentences using cosine similarity.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "def cosine(X, Y):\n",
        "  # tokenization\n",
        "  X_list = word_tokenize(X) \n",
        "  Y_list = word_tokenize(Y)\n",
        "    \n",
        "  stop = stopwords.words('english') \n",
        "  l1 =[]\n",
        "  l2 =[]\n",
        "    \n",
        "  # remove stop words from the string\n",
        "  X_set = {w for w in X_list if not w in stop} \n",
        "  Y_set = {w for w in Y_list if not w in stop}\n",
        "    \n",
        "  # form a set containing keywords of both strings \n",
        "  rvector = X_set.union(Y_set) \n",
        "  for w in rvector:\n",
        "      if w in X_set: l1.append(1) # create a vector\n",
        "      else: l1.append(0)\n",
        "      if w in Y_set: l2.append(1)\n",
        "      else: l2.append(0)\n",
        "  c = 0\n",
        "    \n",
        "  # cosine formula \n",
        "  for i in range(len(rvector)):\n",
        "          c+= l1[i]*l2[i]\n",
        "  cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "  print(\"similarity: \", cosine)"
      ],
      "metadata": {
        "id": "l3vCVvOXEGP4"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}