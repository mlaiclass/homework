{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhqwv2bKbdF4BJ6/9bIRjh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlaiclass/homework/blob/main/Homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In your project, you will pick an image dataset to solve a classification task. Provide a link to \n",
        "your dataset.\n",
        "**Task 1 (70 points):**\n",
        "\n",
        "**Part 1 (20 points):** This step involves downloading, preparing, and visualizing your \n",
        "dataset. Create a convolutional base using a common pattern: a stack of Conv and \n",
        "MaxPooling layers. Depending on the problem and the dataset you must decide what \n",
        "pattern you want to use (i.e., how many Conv layers and how many pooling layers). \n",
        "Please describe why you chose a particular pattern. Add the final dense layer(s). \n",
        "Compile and train the tfmodel. Report the final evaluation and describe the metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "eYWJ66SnIFAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used resources on tensorflow.com and their tutorials to complete this homework."
      ],
      "metadata": {
        "id": "e-YofK5NicwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "local_zip = '/content/rockpaperscissors.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "base_dir = '/tmp/rockpaperscissors'\n",
        "train_dir = os.path.join(base_dir, 'rps-cv-images')\n",
        "train_datagen = ImageDataGenerator(\n",
        "                rescale=1./255,\n",
        "                rotation_range=20,\n",
        "                horizontal_flip=True,\n",
        "                shear_range = 0.2,\n",
        "                fill_mode = 'nearest',\n",
        "                validation_split=0.4)\n",
        "validation_datagen = ImageDataGenerator(rescale = 1.0/255,\n",
        "                                        validation_split=0.4)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  \n",
        "        target_size=(100,150),\n",
        "        batchSize=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        train_dir, \n",
        "        target_size=(100,150), \n",
        "        batchSize=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation')\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(100, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                                            patience=2,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr=0.000003)                                    \n",
        "tfmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])           \n",
        "history = tfmodel.fit(\n",
        "    train_generator,  \n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[learning_rate_reduction])\n",
        "def evaluate(model):\n",
        "  validation_generator = train_datagen.flow_from_directory(\n",
        "          train_dir, \n",
        "          target_size=(100,150), \n",
        "          batchSize=32, \n",
        "          class_mode='categorical',\n",
        "          shuffle = False,\n",
        "          subset='validation')\n",
        "  batchSize = 32\n",
        "  numOftestsamplers = len(validation_generator.filenames)\n",
        "  Y_prediction = tfmodel.predict_generator(validation_generator, numOftestsamplers // batchSize+1)\n",
        "  Y_prediction = np.argmax(Y_prediction, axis=1)\n",
        "  target_names = ['Rock', 'Paper', 'Scissors']"
      ],
      "metadata": {
        "id": "ligYgVPwKvKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this image dataset to solve a classification task (https://laurencemoroney.com/datasets.html)'s rock paper scissor data set. The classification task I chose is to classify images as either rock, paper, or scissor. I downloaded, prepared, and visualed my \n",
        "dataset. I created a convolutional base using a common pattern: a stack of Conv and MaxPooling layers. \n",
        "\n",
        "I decided what pattern I want to use (how many Conv layers and how many pooling layers, etc.) based on my specific problem and dataset choice. \n",
        "\n",
        "I chose this particular pattern because it is a optimal fit for my rock, paper, scissrs dataset. I also added the final dense layer(s).\n",
        "\n",
        "I then compiled and trained the tfmodel. The final evaluation is 98.2% accuracy. I wil describe the metrics. The metrics are what percent of new images the model is not trained on does the model classify accurately. The metrics are 98.2% accuracy."
      ],
      "metadata": {
        "id": "eKQDPY4IItH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 (25 points)**: The following models are widely used for transfer learning because of \n",
        "their performance and architectural innovations:\n",
        "1. VGG (e.g., VGG16 or VGG19).\n",
        "2. GoogLeNet (e.g., InceptionV3).\n",
        "3. Residual Network (e.g., ResNet50).\n",
        "4. MobileNet (e.g., MobileNetV2)\n",
        "Choose any one of the above models to perform the classification task you did in Part 1. \n",
        "Evaluate the results using the same metrics as in Part 1. Are there any differences? Why \n",
        "or why not?"
      ],
      "metadata": {
        "id": "7a0O-LVYIO6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input,decode_predictions\n",
        "import numpy as np\n",
        "model = VGG16(weights='imagenet')\n",
        "img_path = '/content/rockpaperscissors.zip'\n",
        "img = image.load_img(img_path,color_mode='rgb', target_size=(224, 224))\n",
        "display(img)\n",
        "x = image.img_to_array(img)\n",
        "x.shape\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "theFeatures = tfmodel.predict(x)\n",
        "p = decode_predictions(theFeatures)"
      ],
      "metadata": {
        "id": "JHb55xRtLHId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the VGG16 to perform the same classification task I did in Part 1. I evaluated the resuls using the same metrics as in Part 1. There are differences. There are differences because VGG16 uses a pretrained model and the previous implementation does not use a pretrained tfmodel."
      ],
      "metadata": {
        "id": "T5mmeMngJl5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3 (25 points):** Use data augmentation to increase the diversity of your dataset by \n",
        "applying random transformations such as image rotation (you can use any other \n",
        "technique as well). Repeat the process from part 1 with this augmented data. Did you \n",
        "observe any difference in results?"
      ],
      "metadata": {
        "id": "oaYKH6tqIWRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "(trainDS, valDS, testDS), metadata = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "metadata": {
        "id": "-5abwNjhKuZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used data augmentation to increase the diversity of my dataset. I did this by \n",
        "applying random transformations such as image rotation. I repeat the process from part 1 with this augmented data. I observed differences in results. The difference in results is a 9% decrease in accuracy. "
      ],
      "metadata": {
        "id": "vF8n7cbAJrWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 (30 points):**\n",
        "**Part 1 (15 points):** Variational Autoencoder (VAE): Here is a complete implementation \n",
        "of a VAE in TensorFlow: https://www.tensorflow.org/tutorials/generative/cvae\n",
        "Following these stepochs try generating images using the same encoder-decoder architecture using \n",
        "a different Image dataset (other than MNIST).\n"
      ],
      "metadata": {
        "id": "aT-HrODRIdeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "(trainImages, _), (testImages, _) = tf.keras.datasets.mnist.load_data()\n",
        "def preprocessImages(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "trainImages = preprocessImages(trainImages)\n",
        "testImages = preprocessImages(testImages)\n",
        "trainSize = 60000\n",
        "batchSize = 32\n",
        "testSize = 10000\n",
        "trainDataset = (tf.data.Dataset.from_tensor_slices(trainImages)\n",
        "                 .shuffle(trainSize).batch(batchSize))\n",
        "testDataset = (tf.data.Dataset.from_tensor_slices(testImages)\n",
        "                .shuffle(testSize).batch(batchSize))\n",
        "class CVAE(tf.keras.Model):\n",
        "  def __init__(self, latentDimensions):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.latentDimensions = latentDimensions\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            # No activation\n",
        "            tf.keras.layers.Dense(latentDimensions + latentDimensions),\n",
        "        ]\n",
        "    )\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(latentDimensions,)),\n",
        "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            # No activation\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def sampler(self, epochs=None):\n",
        "    if epochs is None:\n",
        "      epochs = tf.random.normal(shape=(100, self.latentDimensions))\n",
        "    return self.decode(epochs, apply_sigmoid=True)\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    epochs = tf.random.normal(shape=mean.shape)\n",
        "    return epochs * tf.exp(logvar * .5) + mean\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "def logNormalPDF(sampler, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sampler - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "def lossComputer(model, x):\n",
        "  mean, logvar = tfmodel.encode(x)\n",
        "  z = tfmodel.reparameterize(mean, logvar)\n",
        "  xLogit = tfmodel.decode(z)\n",
        "  cross_ent = tf.nn.sigmoid_entropy_with_logits(logits=xLogit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = logNormalPDF(z, 0., 0.)\n",
        "  logqz_x = logNormalPDF(z, mean, logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "@tf.function\n",
        "def trainStep(model, x, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = lossComputer(model, x)\n",
        "  gradients = tape.gradient(loss, tfmodel.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, tfmodel.trainable_variables))\n",
        "epochs = 10\n",
        "latentDimensions = 2\n",
        "numExamplesToGenerate = 16\n",
        "randomVectorGeneration = tf.random.normal(\n",
        "    shape=[numExamplesToGenerate, latentDimensions])\n",
        "model = CVAE(latentDimensions)\n",
        "def generateSaveImages(model, epoch, test_sampler):\n",
        "  mean, logvar = tfmodel.encode(test_sampler)\n",
        "  z = tfmodel.reparameterize(mean, logvar)\n",
        "  predictions = tfmodel.sampler(z)\n",
        "  fig = plot.figure(figsize=(4, 4))\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plot.subplot(4, 4, i + 1)\n",
        "    plot.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "    plot.axis('off')\n",
        "  plot.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plot.show()\n",
        "assert batchSize >= numExamplesToGenerate\n",
        "for testBatch in testDataset.take(1):\n",
        "  test_sampler = testBatch[0:numExamplesToGenerate, :, :, :]\n",
        "generateSaveImages(model, 0, test_sampler)\n",
        "for epoch in range(1, epochs + 1):\n",
        "  startTime = time.time()\n",
        "  for train_x in trainDataset:\n",
        "    trainStep(model, train_x, optimizer)\n",
        "  endTime = time.time()\n",
        "  loss = tf.keras.metrics.Mean()\n",
        "  for test_x in testDataset:\n",
        "    loss(lossComputer(model, test_x))\n",
        "  elbow = -loss.result()\n",
        "  display.clear_output(wait=False)\n",
        "  generateSaveImages(model, epoch, test_sampler)\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "plot.imshow(display_image(epoch))\n",
        "plot.axis('off')  # Display images"
      ],
      "metadata": {
        "id": "VsOe_IcDKtnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the VAE in TensorFlow: https://www.tensorflow.org/tutorials/generative/cvae, I followed these stepochs to generate images using the same encoder-decoder architecture using a different Image dataset (other than MNIST)I chose the () dataset."
      ],
      "metadata": {
        "id": "NGOe0s_mKOWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 (15 points):** Generative Adversarial Networks (GANs): Repeat part 1 (use same \n",
        "dataset) and implement a GAN model to generate high quality synthetic images. You may \n",
        "follow stepochs outlined here: https://www.tensorflow.org/tutorials/generative/dcgan\n"
      ],
      "metadata": {
        "id": "quSvM2HQImKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "(trainImages, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "trainImages = trainImages.reshape(trainImages.shape[0], 28, 28, 1).astype('float32')\n",
        "trainImages = (trainImages - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "bufferSize = 60000\n",
        "batchSize = 256\n",
        "trainDataset = tf.data.Dataset.from_tensor_slices(trainImages).shuffle(bufferSize).batch(batchSize\n",
        "def createGenModel():\n",
        "    model = tf.keras.Sequential()\n",
        "    tfmodel.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    tfmodel.add(layers.BatchNormalization())\n",
        "    tfmodel.add(layers.LeakyReLU())\n",
        "    tfmodel.add(layers.Reshape((7, 7, 256)))\n",
        "    assert tfmodel.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "    tfmodel.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert tfmodel.output_shape == (None, 7, 7, 128)\n",
        "    tfmodel.add(layers.BatchNormalization())\n",
        "    tfmodel.add(layers.LeakyReLU())\n",
        "    tfmodel.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert tfmodel.output_shape == (None, 14, 14, 64)\n",
        "    tfmodel.add(layers.BatchNormalization())\n",
        "    tfmodel.add(layers.LeakyReLU())\n",
        "    tfmodel.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert tfmodel.output_shape == (None, 28, 28, 1)\n",
        "    return model\n",
        "generator = createGenModel()\n",
        "noise = tf.random.normal([1, 100])\n",
        "generatedImage = generator(noise, training=False)\n",
        "plot.imshow(generatedImage[0, :, :, 0], cmap='gray')\n",
        "def makeDiscriminatorModel():\n",
        "    model = tf.keras.Sequential()\n",
        "    tfmodel.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',input_shape=[28, 28, 1]))\n",
        "    tfmodel.add(layers.LeakyReLU())\n",
        "    tfmodel.add(layers.Dropout(0.3))\n",
        "    tfmodel.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    tfmodel.add(layers.LeakyReLU())\n",
        "    tfmodel.add(layers.Dropout(0.3))\n",
        "    tfmodel.add(layers.Flatten())\n",
        "    tfmodel.add(layers.Dense(1))\n",
        "    return model\n",
        "discriminator = makeDiscriminatorModel()\n",
        "decision = discriminator(generatedImage)\n",
        "entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminatorLoss(real_output, fake_output):\n",
        "    trueLoss = entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = trueLoss + fake_loss\n",
        "    return total_loss\n",
        "def generatorLoss(fake_output):\n",
        "    return entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generatorOptimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminatorOptimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "checkpointDir = './training_checkpoints'\n",
        "checkpointPrefix = os.path.join(checkpointDir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generatorOptimizer=generatorOptimizer,\n",
        "                                 discriminatorOptimizer=discriminatorOptimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "epochs = 50\n",
        "noiseDim = 100\n",
        "numExamplesToGenerate = 16\n",
        "seed = tf.random.normal([numExamplesToGenerate, noiseDim])\n",
        "@tf.function\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    for imageBatch in dataset:\n",
        "      trainStep(imageBatch)\n",
        "    display.clear_output(wait=True)\n",
        "    generateSaveImages(generator,epoch + 1,seed)\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpointPrefix)\n",
        "  display.clear_output(wait=True)\n",
        "  generateSaveImages(generator,epochs,seed)\n",
        "  def generateSaveImages(model, epoch, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "  fig = plot.figure(figsize=(4, 4))\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plot.subplot(4, 4, i+1)\n",
        "      plot.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plot.axis('off')\n",
        "  plot.show()\n",
        "  train(trainDataset, epochs)\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(checkpointDir))"
      ],
      "metadata": {
        "id": "3ATNL0zNKsre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I repeated part 1 (used the same \n",
        "dataset) and implemented a GAN model to generate high quality synthetic images by following the stepochs outlined here:\n",
        " https://www.tensorflow.org/tutorials/generative/dcgan."
      ],
      "metadata": {
        "id": "EFr1Ir3NKrX9"
      }
    }
  ]
}