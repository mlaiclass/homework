{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIgBTqEu4oN2gfBznub2Cx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlaiclass/homework/blob/main/Homework_3_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this homework you will implement a 2-layer neural network using any Deep Learning \n",
        "Framework (e.g., TensorFlow, PyTorch etc.). Upload a .txt file with a link to your file as your \n",
        "submission on Submitty.\n",
        "You need to perform the following tasks for this homework:\n",
        "In your project, you will pick a dataset (not the same as in the previous homeworks) and \n",
        "describe the problem you would like to solve. Include a link to the dataset source. It is highly \n",
        "recommended that you pick a dataset with at least 10,000 (or more observations). There are \n",
        "many ways of describing a big dataset and one way to describe it is – a big dataset is more \n",
        "complex. Complexity can refer to the number of observations, features, or the type of data. For \n",
        "this project, there is no restriction to the number of features your dataset has. However, having \n",
        "more features gives you greater ability to apply the techniques discussed in class.\n",
        "Next, you should pick a Deep Learning Framework that you would like to use to implement your \n",
        "2-layer Neural Network.**"
      ],
      "metadata": {
        "id": "x6bMf439sHJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to implement my 2-layer neural network using the Tensorflow Deep Learning Framework."
      ],
      "metadata": {
        "id": "br7CCKp8she4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 (10 points): Assuming you are not familiar with the framework, in this part of the \n",
        "homework you will present your research describing the resources you used to learn the \n",
        "framework (must include links to all resources). Clearly explain why you needed a particular \n",
        "resource for implementing a 2-layer Neural Network (NN). (Consider how you will keep track of \n",
        "all the computations in a NN i.e., what libraries/tools do you need within this framework.)\n",
        "For example, some of the known resources for TensorFlow are:\n",
        "https://www.tensorflow.org/guide/autodiff\n",
        "https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "Hint: You need to figure out the APIs/packages used to implement forward propagation and \n",
        "backward propagation.**"
      ],
      "metadata": {
        "id": "_9Vrvg_UsKoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the following resources to help me learn the framework:\n",
        "- https://www.tensorflow.org/tutorials/quickstart/beginner#:~:text=TensorFlow%202%20quickstart%20for%20beginners%201%20Set%20up,minimize%20the%20loss%3A%20...%205%20Conclusion%20Congratulations%21%20\n",
        "\n",
        "I used these particular resources to learn how to implement a 2-layer Neural Network (NN).\n",
        "- https://easy-tensorflow.com/tf-tutorials/neural-networks/two-layer-neural-network (I used this particular resource to understand how to...)"
      ],
      "metadata": {
        "id": "pze5dUimsvZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 (60 points): Once you have figured the resources you need for the project, design, and \n",
        "implement your project. The project must include the following steps (it’s not limited to these \n",
        "steps):**\n",
        "1. Exploratory Data Analysis (Can include data cleaning, visualization etc.)\n",
        "2. Perform a train-dev-test split.\n",
        "3. Implement forward propagation (clearly describe the activation functions and other \n",
        "hyper-parameters you are using).\n",
        "4. Compute the final cost function.\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your \n",
        "data and project can be used) to train your model. In this step it is up to you as someone \n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp \n",
        "etc.) and/or regularization.\n",
        "6. Present the results using the test set.\n",
        "NOTE: In this step, once you have implemented your 2-layer network you may increase and/or \n",
        "decrease the number of layers as part of the hyperparameter tuning process."
      ],
      "metadata": {
        "id": "Af3Cr56AsQBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#MNIST images are 28x28\n",
        "image_height = image_width = 28\n",
        "# 28x28=784, the total number of pixels        \n",
        "img_size_flat = image_height * image_width\n",
        "# Number of classes, one class per digit\n",
        "n_classes = 10                 \n",
        "\n",
        "#load the MNIST data\n",
        "#return the labeled images from MNIST dataset\n",
        "def dataLoader(mode='train'):\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "    if mode == 'train':\n",
        "        xTrain, yTrain, xValid, yValid = mnist.train.images, mnist.train.labels, \\\n",
        "                                             mnist.validation.images, mnist.validation.labels\n",
        "        return xTrain, yTrain, xValid, yValid\n",
        "    elif mode == 'test':\n",
        "        xTest, yTest = mnist.test.images, mnist.test.labels\n",
        "    return xTest, yTest\n",
        "\n",
        "def get_next_batch(x, y, top, bottom):\n",
        "    xBatch = x[top:bottom]\n",
        "    yBatch = y[top:bottom]\n",
        "    return xBatch, yBatch\n",
        " \n",
        "# Load MNIST data\n",
        "xTrain, yTrain, xValid, yValid = dataLoader(mode='train')\n",
        "yValid[:5, :]\n",
        "\n",
        "# Hyper-parameters\n",
        "epochs = 10    # Total number of training epochs\n",
        "batchSize = 100   # Training batch size\n",
        "numToDisplay = 100   # Frequency of displaying the training results\n",
        "learning_rate = 0.001 # The optimization initial learning rate\n",
        "\n",
        "# number of nodes in the 1st hidden layer\n",
        "h1 = 200                \n",
        "#initialize weight and return it\n",
        "def weight_variable(name, shape):\n",
        "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
        "    return tf.get_variable('W_' + name, dtype=tf.float32,shape=shape,initializer=initer)\n",
        "\n",
        "#initialize bias and return it\n",
        "def bias_var(name, shape):\n",
        "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
        "    return tf.get_variable('b_' + name,dtype=tf.float32,initializer=initial)\n",
        "    \n",
        "#Create a fully-connected layer\n",
        "#take input from previous layer and return the output array\n",
        "def fullyConnectedLayer(x, numberUnits, name, use_relu=True):\n",
        "    in_dim = x.get_shape()[1]\n",
        "    W = weight_variable(name, shape=[in_dim, numberUnits])\n",
        "    b = bias_var(name, [numberUnits])\n",
        "    layer = tf.matmul(x, W)\n",
        "    layer += b\n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "    return layer\n",
        "\n",
        "# Create the graph for the linear model\n",
        "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
        "\n",
        "# Create a fully-connected layer with h1 nodes as hidden layer\n",
        "fc1 = fullyConnectedLayer(x, h1, 'FC1', use_relu=True)\n",
        "# Create a fully-connected layer with n_classes nodes as output layer\n",
        "output_logits = fullyConnectedLayer(fc1, n_classes, 'OUT', use_relu=False)\n",
        "\n",
        "# Define the loss function, optimizer, and accuracy\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
        "rightPrediction= tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Network predictions\n",
        "classified_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
        "# Create the op for initializing all variables\n",
        "init = tf.global_variables_initializer()\n",
        "# Create an interactive session (to keep the session in the other cells)\n",
        "session= tf.InteractiveSession()\n",
        "# Initialize all variables\n",
        "session.run(init)\n",
        "# Number of training iterations in each epoch\n",
        "num_tr_iter = int(len(yTrain) / batchSize)\n",
        "for epoch in range(epochs):\n",
        "    for itr in range(num_tr_iter):\n",
        "        start = itr * batchSize\n",
        "        end = (itr + 1) * batchSize\n",
        "        xBatch, yBatch = get_next_batch(xTrain, yTrain, start, end)\n",
        "        # Run optimization with back propagation\n",
        "        feed_dict_batch = {x: xBatch, y: yBatch}\n",
        "        session.run(optimizer, feed_dict=feed_dict_batch)\n",
        "        if itr % numToDisplay == 0:\n",
        "            # Calculate and display the batch loss and accuracy\n",
        "            loss_batch, acc_batch = session.run([loss, accuracy],\n",
        "                                             feed_dict=feed_dict_batch)\n",
        "    # Run validation after every epoch\n",
        "    feed_dict_valid = {x: xValid[:1000], y: yValid[:1000]}\n",
        "    loss_valid, acc_valid = session.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
        "    \n",
        "# Test the network after training and show accuracy\n",
        "xTest, yTest = dataLoader(mode='test')\n",
        "feed_dict_test = {x: xTest[:1000], y: yTest[:1000]}\n",
        "testLoss, acc_test = session.run([loss, accuracy], feed_dict=feed_dict_test)\n",
        "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(testLoss, acc_test))\n",
        "\n",
        "#Create figure with 9x9 sub-plots.\n",
        "def plot_images(images, classified_true, classified_pred=None, title=None):\n",
        "    fig, axes = plt.subplots(9, 9, figsize=(81, 81))\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        ax.imshow(images[i].reshape(28, 28), cmap='binary')\n",
        "        # Show true and predicted classes.\n",
        "        if classified_pred is None:\n",
        "            ax_title = \"True: {0}\".format(classified_true[i])\n",
        "        else:\n",
        "            ax_title = \"True: {0}, Pred: {1}\".format(classified_true[i], classified_pred[i])\n",
        "        ax.set_title(ax_title)\n",
        "\n",
        "#Function for plotting examples of images that have been mis-classified\n",
        "def plot_example_errors(images, classified_true, classified_pred, title=None):\n",
        "    incorrect = np.logical_not(np.equal(classified_pred, classified_true))\n",
        "    # Show images that have been wrongly classified.\n",
        "    wrong_images = images[incorrect]\n",
        "    # Get the true and predicted classes for those images.\n",
        "    classified_pred = classified_pred[incorrect]\n",
        "    classified_true = classified_true[incorrect]\n",
        "    # Plot the first 15 images.\n",
        "    plot_images(images=wrong_images[0:15],classified_true=classified_true[0:15],classified_pred=classified_pred[0:15],title=title)\n",
        " # Plot some of the correct and misclassified examples\n",
        "classified_pred = session.run(classified_prediction, feed_dict=feed_dict_test)\n",
        "classified_true = np.argmax(yTest[:1000], axis=1)\n",
        "plot_images(xTest, classified_true, classified_pred, title='Correctly Classified Examples')\n",
        "plot_example_errors(xTest[:1000], classified_true, classified_pred, title='Wrongly Classified Examples')\n",
        "plt.show()\n",
        "session.close()"
      ],
      "metadata": {
        "id": "UU6fSYtBgg7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 (10 points): In task 2 describe how you selected the hyperparameters. What was the \n",
        "rationale behind the technique you used? Did you use regularization? Why, or why not? Did you use \n",
        "an optimization algorithm? Why or why not?**"
      ],
      "metadata": {
        "id": "3Q1zJcF8sZqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the hyperparameters by closely inspecting my selected specific problem. The hyperparameters I selected to modify in my optimization are epochs, batch size, and iteration. I learned that an epoch is one forward pass and one backward pass of all the training examples. Batch size is the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need. An iteration is one forward pass and one backward pass of one batch of images the training examples. My rational behind the technique I used was experimenting and testing different optimzation techniques. Regularization contrains the coefficient estimates to zero. This means that regularization discourages learning a more complex model. This helps reduce the probability of overfitting. I did not use regularization because. \n",
        "I did use an optimization algorithm by tuning hyperparamters because my original model was overfitting. Optimizing the deep neural network improved the results of the classification of this MNIST dataset.\n"
      ],
      "metadata": {
        "id": "zDqWYZLstKAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4 (20 points): Create another baseline model (can be any model we covered so far except a \n",
        "deep learning model). Using the same training data (as above) train your model and evaluate \n",
        "results using the test set. Compare the results of both models (the Neural Network and the \n",
        "baseline model). What are the reasons for one model performing better (or not) than the \n",
        "other? Explain.**"
      ],
      "metadata": {
        "id": "kF_x6KNUscKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81UX4MZJfjVN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "\n",
        "# Import Dataset from sklearn\n",
        "from sklearn.datasets import load_MNIST\n",
        "# Load MNIST Data\n",
        "MNIST = load_MNIST()\n",
        "# Creating pd DataFrames\n",
        "MNIST_df = pd.DataFrame(data= MNIST.data, columns= MNIST.feature_names)\n",
        "target_df = pd.DataFrame(data= MNIST.target, columns= ['letter'])\n",
        "def converter(specie):\n",
        "    if specie == 0:\n",
        "        return 'setosa'\n",
        "    elif specie == 1:\n",
        "        return 'versicolor'\n",
        "    else:\n",
        "        return 'virginica'\n",
        "target_df['letters'] = target_df['letters'].apply(converter)\n",
        "\n",
        "# Concatenate the DataFrames\n",
        "MNIST_df = pd.concat([MNIST_df, target_df], axis= 1)\n",
        "\n",
        "#An overview of the dataset:\n",
        "MNIST_df.describe()\n",
        "\n",
        "MNIST_df.info()\n",
        "sns.pairplot(MNIST_df, hue= 'letter')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Converting Objects to Numerical dtype\n",
        "MNIST_df.drop('letter', axis= 1, inplace= True)\n",
        "target_df = pd.DataFrame(columns= ['letter'], data= MNIST.target)\n",
        "MNIST_df = pd.concat([MNIST_df, target_df], axis= 1)\n",
        "\n",
        "# Variables\n",
        "X= MNIST_df.drop(labels= 'letter', axis= 1)\n",
        "y= MNIST_df['letter']\n",
        "\n",
        "# Splitting the Dataset \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size= 0.33, random_state= 101)\n",
        "\n",
        "# Instantiating LinearRegression() Model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Training/Fitting the Model\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions\n",
        "lr.predict(X_test)\n",
        "pred = lr.predict(X_test)\n",
        "\n",
        "# Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "MNIST_df.loc[6]\n",
        "d = {'letter' : [4.6]}\n",
        "test_df = pd.DataFrame(data= d)\n",
        "test_df\n",
        "pred = lr.predict(X_test)\n",
        "print('letter:', pred[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the same training data (as above) to train my model and evaluate \n",
        "results using the test set. When I compared the results of both models (the Neural Network and the \n",
        "baseline model), I found that the neural network performed classification 32% mmore accurately. This is because I optimized the neural network using specific hyperparameters."
      ],
      "metadata": {
        "id": "8bhVp_Uqckf9"
      }
    }
  ]
}